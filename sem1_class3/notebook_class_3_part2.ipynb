{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**План семинара** \"Создание собственных операторов Pytorch-Python. Часть 2 - Attention Mechanisms\n",
        "\n",
        "1) Vanilla Attention [paper 2015](https://arxiv.org/abs/1409.0473), [paper 2015](https://arxiv.org/abs/1508.04025)\n",
        "\n",
        "2) Self-Attention, Simplified Self-Attention [Attention Is All You Need 2017](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "3) External Attention [paper 2021](https://arxiv.org/abs/2105.02358)\n",
        "\n",
        "4) Attention Mechanisms in Computer Vision:\n",
        "A Survey [paper 2021](https://arxiv.org/pdf/2111.07624.pdf)"
      ],
      "metadata": {
        "id": "92EBgUdJZIdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vanilla Attention"
      ],
      "metadata": {
        "id": "F25XDygxb9W5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Полезные ссылки:\n",
        "\n",
        "1) [Общее описание + представление данных + лосс функция. Классический RNN Encoder-Decoder](https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b)\n",
        "\n",
        "2) [Описание статьи с изображения снизу](https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/)"
      ],
      "metadata": {
        "id": "Lo7CFDrdQbEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Прочитать статью:** пункт 2.1, 3.1, 3.2\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1fWp4x3LFZbWXq77VQoBUBNWbvmd7qH_3)\n",
        "\n",
        "Изображение взято с [paper](https://arxiv.org/pdf/1703.03906.pdf), [свободное описание](https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/)\n"
      ],
      "metadata": {
        "id": "gWOPDcCgcBAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вопросы:\n",
        "\n",
        "1) Что из себя представляеТ классический RNN Encoder-Decoder?\n",
        "\n",
        "2) Чему равен вектор с в классическом RNN Encoder-Decoder?\n"
      ],
      "metadata": {
        "id": "1W9bcolLHqbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1V3QsG5nCVZI8tVVyqi4B8aEFG5qwQ6tw)\n"
      ],
      "metadata": {
        "id": "SKKePDGQ0AXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention query - Si (decoder_state)\n",
        "\n",
        "attention key - Hj (encode state)\n",
        "\n",
        "From [this paper, 4.5](https://arxiv.org/pdf/1703.03906.pdf)"
      ],
      "metadata": {
        "id": "7KLcAbwKToot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LglUSJ3SeDUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the attention network proposed in [1] and [2].\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim : int\n",
        "        Size of the input tensor\n",
        "    References\n",
        "    ----------\n",
        "    1. \"`Neural Machine Translation by Jointly Learning to Align and Translate. \\\n",
        "            <https://arxiv.org/abs/1409.0473>`_\" Dzmitry Bahdanau, et al. ICLR 2015.\n",
        "    2. \"`Effective Approaches to Attention-based Neural Machine Translation. \\\n",
        "            <https://arxiv.org/abs/1508.04025>`_\" Minh-Thang Luong, et al. EMNLP 2015.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "    ) -> None:\n",
        "        super(VanillaAttention, self).__init__()\n",
        "\n",
        "        self.fc_align = nn.Linear(..)\n",
        "\n",
        "        self.fc_query = nn.Linear(...)\n",
        "        self.fc_value = nn.Linear(...)\n",
        "\n",
        "        self.softmax = nn.Softmax(...)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self, query: torch.Tensor, key: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        query : torch.Tensor (batch_size, dim)\n",
        "            Query\n",
        "        key : torch.Tensor (batch_size, length, dim)\n",
        "            Key\n",
        "        Returns\n",
        "        -------\n",
        "        out : torch.Tensor (batch_size, dim)\n",
        "            Output tensor\n",
        "        att: torch.Tensor (batch_size, length)\n",
        "            Attention weights\n",
        "        \"\"\"\n",
        "\n",
        "        # alignment scores\n",
        "        score = self.fc_align(...)\n",
        "        score = ...  \n",
        "\n",
        "        # attention weights\n",
        "        att = ...\n",
        "\n",
        "        # context vector (weighted key)\n",
        "        context = ...\n",
        "\n",
        "        # attention result\n",
        "        out = self.tanh(...)\n",
        "\n",
        "        return out, att"
      ],
      "metadata": {
        "id": "3zND19Z54Tmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = VanillaAttention(2)\n",
        "query = torch.rand(20, 2)\n",
        "key = torch.rand(20, 8, 2)"
      ],
      "metadata": {
        "id": "bhcraZPSKnih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = attention(query, key)\n",
        "print(res[0].shape)\n",
        "print(res[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JjZX_KuJdQo",
        "outputId": "18a343ad-8e6b-4b15-9278-11ce5e7827c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 2])\n",
            "torch.Size([20, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is all you need"
      ],
      "metadata": {
        "id": "tMGo_Qc_PQK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Прочитать**:\n",
        "\n",
        "**Вопросы**:\n",
        "\n",
        "1) Отличие RNN от Transformer. Как подаются данные и как это связано с backprop?\n",
        "\n",
        "2) Зачем нужен positional encoding?\n",
        "\n",
        "3) Практическое значение skip-connections? Какую именно полезную ифнформацию таким образом мы передаем сохраняем?\n",
        "\n",
        "4) Понятны практические значения V, K, Q в Multi-Head Attention при соединении Encoder-Decoder?\n",
        "\n",
        "5) Формула attention - геометрическое объяснение формулы."
      ],
      "metadata": {
        "id": "LtemtmotdwE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "BcoPSh8aaYII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_heads(x: torch.Tensor, n_heads: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor (batch_size, length, dim)\n",
        "        Input tensor.\n",
        "    n_heads : int\n",
        "        Number of attention heads.\n",
        "    \"\"\"\n",
        "    batch_size, dim = x.size(0), x.size(-1)\n",
        "    x = x.view(batch_size, -1, n_heads, dim // n_heads)  # (batch_size, length, n_heads, d_head)\n",
        "    x = x.transpose(1, 2)  # (batch_size, n_heads, length, d_head)\n",
        "    return x\n",
        "\n",
        "def combine_heads(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor (batch_size, n_heads, length, d_head)\n",
        "        Input tensor.\n",
        "    \"\"\"\n",
        "    #https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch\n",
        "    batch_size, n_heads, d_head = x.size(0), x.size(1), x.size(3)\n",
        "    x = x.transpose(1, 2).contiguous().view(batch_size, -1, d_head * n_heads)  # (batch_size, length, n_heads * d_head)\n",
        "    return x\n",
        "\n",
        "def add_mask(x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Mask away by setting such weights to a large negative number, so that they evaluate to 0\n",
        "    under the softmax.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor (batch_size, n_heads, *, length) or (batch_size, length)\n",
        "        Input tensor.\n",
        "    mask : torch.Tensor, optional (batch_size, length)\n",
        "        Mask metrix, ``None`` if it is not needed.\n",
        "    \"\"\"\n",
        "    if mask is not None:\n",
        "        if len(x.size()) == 4:\n",
        "            expanded_mask = mask.unsqueeze(1).unsqueeze(1)  # (batch_size, 1, 1, length)\n",
        "        x = x.masked_fill(expanded_mask.bool(), -np.inf)\n",
        "    return "
      ],
      "metadata": {
        "id": "gycZqcDPaf8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention\n",
        "    Parameters\n",
        "    ----------\n",
        "    scale : float\n",
        "        Scale factor (``sqrt(d_head)``).\n",
        "    dropout : float, optional\n",
        "        Dropout, ``None`` if no dropout layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, scale: float, dropout: float = 0.5) -> None:\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "        self.scale = scale\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = None if dropout is None else nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        Q: torch.Tensor,\n",
        "        K: torch.Tensor,\n",
        "        V: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        Q : torch.Tensor (batch_size, n_heads, length, d_head)\n",
        "            Query\n",
        "        K : torch.Tensor (batch_size, n_heads, length, d_head)\n",
        "            Key \n",
        "        V : torch.Tensor (batch_size, n_heads, length, d_head)\n",
        "            Value\n",
        "        mask : torch.Tensor (batch_size, 1, 1, length)\n",
        "            Mask metrix, None if it is not needed\n",
        "        Returns\n",
        "        -------\n",
        "        context : torch.Tensor (batch_size, n_heads, length, d_head)\n",
        "            Context vector.\n",
        "        att : torch.Tensor (batch_size, n_heads, length, length)\n",
        "            Attention weights.\n",
        "        \"\"\"\n",
        "        # Q·K^T / sqrt(d_head)\n",
        "        score = ...\n",
        "        score = ... # add_mask\n",
        "\n",
        "        # eq.1: Attention(Q, K, V) = softmax(Q·K^T / sqrt(d_head))·V\n",
        "        att = ...\n",
        "        att = ... # dropout\n",
        "        context = ...\n",
        "\n",
        "        return context, att"
      ],
      "metadata": {
        "id": "g9OkQNTlPVDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Multi-Head Self-Attention proposed in [1].\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim : int\n",
        "        Dimension of the input features.\n",
        "    n_heads : int\n",
        "        Number of attention heads.\n",
        "    dropout : float, optional\n",
        "        Dropout, ``None`` if no dropout layer.\n",
        "    References\n",
        "    ----------\n",
        "    1. \"`Attention Is All You Need. <https://arxiv.org/abs/1706.03762>`_\" Ashish Vaswani, et al. NIPS 2017.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        n_heads: int = 8,\n",
        "        dropout: Optional[float] = None\n",
        "    ) -> None:\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        assert dim % n_heads == 0\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = dim // n_heads\n",
        "        \n",
        "        # linear projections\n",
        "        self.W_Q = nn.Linear(...)\n",
        "        self.W_K = nn.Linear(...)\n",
        "        self.W_V = nn.Linear(...)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scale = self.d_head ** 0.5  # scale factor\n",
        "        self.attention = ScaledDotProductAttention(scale=scale, dropout=dropout)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(dim)\n",
        "        self.fc = nn.Linear(...)\n",
        "\n",
        "        self.dropout = None if dropout is None else nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor (batch_size, length, dim)\n",
        "            Input data, where ``length`` is the length (number of features) of the input and\n",
        "            ``dim`` is the dimension of the features.\n",
        "        mask : torch.Tensor, optional (batch_size, length)\n",
        "            Mask metrix, ``None`` if it is not needed.\n",
        "        Returns\n",
        "        -------\n",
        "        out : torch.Tensor (batch_size, length, dim)\n",
        "            Output of multi-head self-attention network.\n",
        "        \"\"\"\n",
        "        Q = ...\n",
        "        K = ...\n",
        "        V = ...\n",
        "\n",
        "        Q, K, V = split_heads(Q, self.n_heads), split_heads(K, self.n_heads), split_heads(V, self.n_heads)  # (batch_size, n_heads, length, d_head)\n",
        "\n",
        "        context, _ = ...\n",
        "        context = combine_heads(context)  # (batch_size, length, n_heads * d_head)\n",
        "\n",
        "        out = ...\n",
        "        out = out if self.dropout is None else self.dropout(out)\n",
        "\n",
        "        out = ... # residual connection\n",
        "        out = self.layer_norm(out)  # LayerNorm\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "cRCHqyFoalmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedSelfAttention(SelfAttention):\n",
        "    \"\"\"\n",
        "    Implementation of a common simplified version of Multi-Head Self-Attention, which drops the\n",
        "    linear projection layers and directly calculates an attention map from the input feature to\n",
        "    reduce the computational complexity.\n",
        "    ----------\n",
        "    dim : int\n",
        "        Dimension of the input features.\n",
        "    n_heads : int\n",
        "        Number of attention heads.\n",
        "    dropout : float, optional\n",
        "        Dropout, ``None`` if no dropout layer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        n_heads: int = 8,\n",
        "        dropout: Optional[float] = None\n",
        "    ) -> None:\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        ....\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor (batch_size, length, dim)\n",
        "            Input data, where ``length`` is the length (number of features) of the input and\n",
        "            ``dim`` is the dimension of the features.\n",
        "        mask : torch.Tensor, optional (batch_size, length)\n",
        "            Mask metrix, ``None`` if it is not needed.\n",
        "        Returns\n",
        "        -------\n",
        "        out : torch.Tensor (batch_size, length, dim)\n",
        "            Output of multi-head self-attention network.\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "WcEE2hXsahtn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}