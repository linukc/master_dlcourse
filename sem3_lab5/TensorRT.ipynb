{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwCcG053Tqu9",
        "outputId": "f139d161-4c11-4f3a-c0a6-1ffd0965bf25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                            2.0.1+cu118\n",
            "torchaudio                       2.0.2+cu118\n",
            "torchdata                        0.6.1\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.15.2\n",
            "torchvision                      0.15.2+cu118\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWi414FtaHyl",
        "outputId": "19a1e09a-48e1-4874-ff55-dc41cdd56293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGpqdMkNkVuU",
        "outputId": "4ce79a23-8305-424a-f8f3-32311d31068b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct  1 19:37:48 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    27W /  70W |   1393MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt6CtCdNcOPX",
        "outputId": "9a46253e-6cf1-4784-a5b0-88912a35eb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#define CUDNN_MAJOR 8\n",
            "#define CUDNN_MINOR 9\n",
            "#define CUDNN_PATCHLEVEL 0\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "/* cannot use constexpr here since this is a C-only file */\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorrt\n",
        "!pip list | grep tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWBLO6BxcdHw",
        "outputId": "0b3ed6d0-bfb1-4bd1-a10a-1dc4005bf805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorrt\n",
            "  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tensorrt\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-8.6.1.post1-py2.py3-none-any.whl size=17281 sha256=98680342106bfb7b07061b763770ed41c76bd08db680ffc05afc385c170705bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c8/0e/b79b08e45752491b9acfdbd69e8a609e8b2ed7640dda5a3e59\n",
            "Successfully built tensorrt\n",
            "Installing collected packages: tensorrt\n",
            "Successfully installed tensorrt-8.6.1.post1\n",
            "tensorrt                         8.6.1.post1\n",
            "tensorrt-bindings                8.6.1\n",
            "tensorrt-libs                    8.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Docker: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorrt"
      ],
      "metadata": {
        "id": "BK6aDJeof4gK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Pytorch + TensorRT](https://pytorch.org/TensorRT/)"
      ],
      "metadata": {
        "id": "fi0rwgBxafFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "Vhnsov2xjDRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install nvidia-pyindex\n",
        "!pip3 install nvidia-tensorrt\n",
        "!pip3 install torch-tensorrt==1.4.0 -f https://github.com/pytorch/TensorRT/releases/expanded_assets/1.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t8OOrMCfrhR",
        "outputId": "a255a943-1b68-44bd-d2b4-84c88bfadf75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-pyindex\n",
            "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8417 sha256=2c861d71196d3ac9b184b6214544caea879a442264745e57419b19172d315e17\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/af/d0/7a12f82cab69f65d51107f48bcd6179e29b9a69a90546332b3\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.9\n",
            "Collecting nvidia-tensorrt\n",
            "  Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl (17 kB)\n",
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.10/dist-packages (from nvidia-tensorrt) (8.6.1.post1)\n",
            "Installing collected packages: nvidia-tensorrt\n",
            "Successfully installed nvidia-tensorrt-99.0.0\n",
            "Looking in links: https://github.com/pytorch/TensorRT/releases/expanded_assets/1.4.0\n",
            "Collecting torch-tensorrt==1.4.0\n",
            "  Downloading torch_tensorrt-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.1,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from torch-tensorrt==1.4.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.1,>=2.0.1->torch-tensorrt==1.4.0) (1.3.0)\n",
            "Installing collected packages: torch-tensorrt\n",
            "Successfully installed torch-tensorrt-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Torch-TensorRT in Python"
      ],
      "metadata": {
        "id": "rUfXpRtQr-Zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch-TensorRT Python API can accept a torch.nn.Module, torch.jit.ScriptModule, or torch.fx.GraphModule as an input"
      ],
      "metadata": {
        "id": "GOYHE9cXxFGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/TensorRT/py_api/torch_tensorrt.html"
      ],
      "metadata": {
        "id": "jvi190v2xaFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_tensorrt\n",
        "import torch"
      ],
      "metadata": {
        "id": "JF9MwLoasBRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = models.mobilenet_v2(pretrained=True).eval().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r852JdtotVip",
        "outputId": "40034a47-7088-4bd2-a547-b012fb99a5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 64.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "kOvXQvMMvNLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = torch.rand([1, 3, 224, 224]).to(device)\n",
        "for _ in range(10):\n",
        "  t0 = time.time()\n",
        "  model(input_)\n",
        "  torch.cuda.current_stream().synchronize()\n",
        "  t1 = time.time()\n",
        "  print(t1 - t0)"
      ],
      "metadata": {
        "id": "1hseK5QMvQOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58341424-eca0-43f6-babd-511d1e54d6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.013536214828491211\n",
            "0.007731199264526367\n",
            "0.008272647857666016\n",
            "0.007344722747802734\n",
            "0.008091211318969727\n",
            "0.007662057876586914\n",
            "0.0074388980865478516\n",
            "0.007418155670166016\n",
            "0.0073130130767822266\n",
            "0.0071849822998046875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [\n",
        "    torch_tensorrt.Input(\n",
        "        min_shape=[1, 3, 224, 224],\n",
        "        opt_shape=[1, 3, 224, 224],\n",
        "        max_shape=[1, 3, 224, 224],\n",
        "        dtype=torch.half,\n",
        "    )\n",
        "]\n",
        "enabled_precisions = {torch.float, torch.half}  # Run with fp16\n",
        "\n",
        "trt_ts_module = torch_tensorrt.compile(\n",
        "    model, inputs=inputs, enabled_precisions=enabled_precisions\n",
        ")\n"
      ],
      "metadata": {
        "id": "Da0sAQQvv3-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = torch.randn((1, 3, 224, 224)).to(\"cuda\").half()\n",
        "for _ in range(10):\n",
        "  t0 = time.time()\n",
        "  result = trt_ts_module(input_data)\n",
        "  torch.cuda.current_stream().synchronize()\n",
        "  t1 = time.time()\n",
        "  print(t1 - t0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVax4448s17V",
        "outputId": "d1a3ef5c-8ce1-4ede-fac1-36a6beaeb16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07064247131347656\n",
            "0.008928537368774414\n",
            "0.0011997222900390625\n",
            "0.0011873245239257812\n",
            "0.001203775405883789\n",
            "0.001194000244140625\n",
            "0.0011894702911376953\n",
            "0.0011870861053466797\n",
            "0.0011799335479736328\n",
            "0.0012106895446777344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.jit.save(trt_ts_module, \"trt_ts_module.ts\")"
      ],
      "metadata": {
        "id": "ictUrte6v8mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deployment application\n",
        "import torch\n",
        "import torch_tensorrt\n",
        "\n",
        "trt_ts_module = torch.jit.load(\"trt_ts_module.ts\")\n",
        "input_data = input_data.to(\"cuda\").half()\n",
        "t0 = time.time()\n",
        "result = trt_ts_module(input_data)\n",
        "torch.cuda.current_stream().synchronize()\n",
        "t1 = time.time()\n",
        "print(t1 - t0)"
      ],
      "metadata": {
        "id": "4BaZGQzbtuJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37276cd1-09f3-4761-958a-71869537f86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0020499229431152344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Torch-TensorRT Directly From PyTorch"
      ],
      "metadata": {
        "id": "dSL7SYtijJxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_tensorrt # import require GPU"
      ],
      "metadata": {
        "id": "6rzOsnayjwBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = models.mobilenet_v2(pretrained=True).eval().to(device)\n",
        "script_model = torch.jit.script(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwsl1dUfjyFM",
        "outputId": "e6f3c9ec-5848-45f5-8f10-5cb159496641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### https://pytorch.org/TensorRT/py_api/ts.html\n",
        "spec = {\n",
        "    \"forward\": torch_tensorrt.ts.TensorRTCompileSpec(\n",
        "        **{\n",
        "            \"inputs\": [torch_tensorrt.Input([1, 3, 224, 224])],\n",
        "            \"enabled_precisions\": {torch.half},\n",
        "            \"refit\": False,\n",
        "            \"debug\": False,\n",
        "            \"device\": {\n",
        "                \"device_type\": torch_tensorrt.DeviceType.GPU,\n",
        "                \"gpu_id\": 0,\n",
        "                \"dla_core\": 0,\n",
        "                \"allow_gpu_fallback\": True,\n",
        "            },\n",
        "            \"capability\": torch_tensorrt.EngineCapability.default,\n",
        "            \"num_avg_timing_iters\": 1,\n",
        "        }\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "qmMgQ_2tk_R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trt_model = torch._C._jit_to_backend(\"tensorrt\", script_model, spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd9boA2plgfI",
        "outputId": "02dd6d46-3b27-48f9-e25e-f2a06d1e139e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [Torch-TensorRT TorchScript Conversion Context] - CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
            "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
            "WARNING: [Torch-TensorRT TorchScript Conversion Context] - If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
            "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Check verbose logs for the list of affected weights.\n",
            "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 50 weights are affected by this issue: Detected subnormal FP16 values.\n",
            "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 4 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
            "WARNING: [Torch-TensorRT] - CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### https://github.com/pytorch/TensorRT/issues/2113\n",
        "input = torch.randn((1, 3, 224, 224)).to(\"cuda\").to(torch.float)\n",
        "trt_model.forward(input).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLLb3_iDlkAH",
        "outputId": "70cf3263-990f-46a5-84d1-2a04397f522b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(trt_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9kazCaMnqC8",
        "outputId": "54e6f268-5d6a-45d6-b997-6f0ccfc83e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.jit._script.RecursiveScriptModule"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post Training Quantization (PTQ)"
      ],
      "metadata": {
        "id": "ox6kEse8w9lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "tk9Z0pn9zHcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "testing_dataloader = torch.utils.data.DataLoader(\n",
        "    testing_dataset, batch_size=1, shuffle=False, num_workers=1\n",
        ")\n",
        "calibrator = torch_tensorrt.ptq.DataLoaderCalibrator(\n",
        "    testing_dataloader,\n",
        "    cache_file=\"./calibration.cache\",\n",
        "    use_cache=False,\n",
        "    algo_type=torch_tensorrt.ptq.CalibrationAlgo.ENTROPY_CALIBRATION_2,\n",
        "    device=torch.device(\"cuda:0\"),\n",
        ")\n",
        "model = models.mobilenet_v2(pretrained=True).eval().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PD7IpNJzA_r",
        "outputId": "94fc770b-8b6d-427e-e97e-6cfa68ef5313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44847432.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trt_mod = torch_tensorrt.compile(model, inputs=[torch_tensorrt.Input((1, 3, 32, 32))],\n",
        "                                    enabled_precisions={torch.float, torch.half, torch.int8},\n",
        "                                    calibrator=calibrator,\n",
        "                                    device={\n",
        "                                         \"device_type\": torch_tensorrt.DeviceType.GPU,\n",
        "                                         \"gpu_id\": 0,\n",
        "                                         \"dla_core\": 0,\n",
        "                                         \"allow_gpu_fallback\": False,\n",
        "                                         \"disable_tf32\": False\n",
        "                                     })"
      ],
      "metadata": {
        "id": "OHYHJTROzQUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = torch.randn((1, 3, 32, 32)).to(\"cuda\")\n",
        "for _ in range(10):\n",
        "  t0 = time.time()\n",
        "  result = trt_mod(input_data)\n",
        "  torch.cuda.current_stream().synchronize()\n",
        "  t1 = time.time()\n",
        "  print(t1 - t0)"
      ],
      "metadata": {
        "id": "SmkzYJcbzuHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trt_mod.save(\"ptq.ts\")"
      ],
      "metadata": {
        "id": "yk1kgYTy0V3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch - ONNX - Tensorrt"
      ],
      "metadata": {
        "id": "4i2fP1tE5TJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NVIDIA/TensorRT/blob/main/quickstart/IntroNotebooks/4.%20Using%20PyTorch%20through%20ONNX.ipynb"
      ],
      "metadata": {
        "id": "JPoc6VIa5X-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorRT + Nvidia pytorch_quantization"
      ],
      "metadata": {
        "id": "KZW_4Qj-2Uq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization"
      ],
      "metadata": {
        "id": "5sHxd7jG2dsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NVIDIA/TensorRT/blob/release/8.6/quickstart/quantization_tutorial/qat-ptq-workflow.ipynb"
      ],
      "metadata": {
        "id": "-DZIg37Prvsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "7JhLg3U27GCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача на лабораторную"
      ],
      "metadata": {
        "id": "7whA1SEeve1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Провести QAT для выбранной модели и датасета и представить результаты сравнения с лабораторной 1. Опционально рассказать о трудностях, которые возникли при работе с tensorrt."
      ],
      "metadata": {
        "id": "emVj_N82vm7Q"
      }
    }
  ]
}