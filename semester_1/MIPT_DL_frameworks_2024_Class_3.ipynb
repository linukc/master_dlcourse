{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\" width=\"100%\">\n",
        "    <img width=\"66%\" src=\"https://raw.githubusercontent.com/linukc/master_dlcourse/main/images/logo.png\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "n3OPFlk49eqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **[MIPT DL frameworks Spring 2024](https://wiki.cogmodel.mipt.ru/s/mtai/doc/2024-nejrosetevye-frejmvorki-glubokogo-obucheniya-ZBGd69bxLd). Class 3: building deep learning blocks**"
      ],
      "metadata": {
        "id": "f1gUzca29g-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "sKzK2g7tB_tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels))\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1PwLr2AhCA6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inception"
      ],
      "metadata": {
        "id": "o66x2_fnAfyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WFpQPQVYiMe"
      },
      "outputs": [],
      "source": [
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_1x1,\n",
        "        red_3x3,\n",
        "        out_3x3,\n",
        "        red_5x5,\n",
        "        out_5x5,\n",
        "        out_pool,\n",
        "    ):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        self.branch1 = ConvBlock(in_channels, out_1x1, kernel_size=1)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            ConvBlock(in_channels, red_3x3, kernel_size=1, padding=0),\n",
        "            ConvBlock(red_3x3, out_3x3, kernel_size=3, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            ConvBlock(in_channels, red_5x5, kernel_size=1),\n",
        "            ConvBlock(red_5x5, out_5x5, kernel_size=5, padding=2),\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
        "            ConvBlock(in_channels, out_pool, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branches = (self.branch1, self.branch2, self.branch3, self.branch4)\n",
        "        return torch.cat([branch(x) for branch in branches], dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depthwise Separable Convolution"
      ],
      "metadata": {
        "id": "2NMDERKXCLZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "sa0sg5T9QTHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Mixer"
      ],
      "metadata": {
        "id": "2Mga2OV0ROG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops.layers.torch import Rearrange"
      ],
      "metadata": {
        "id": "65zoAKrvRPrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MixerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_patch, token_dim, channel_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_mix = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            Rearrange('b n d -> b d n'),\n",
        "            FeedForward(num_patch, token_dim, dropout),\n",
        "            Rearrange('b d n -> b n d')\n",
        "        )\n",
        "\n",
        "        self.channel_mix = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            FeedForward(dim, channel_dim, dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + self.token_mix(x)\n",
        "\n",
        "        x = x + self.channel_mix(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "76VQZ6UkRSwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patch Merger"
      ],
      "metadata": {
        "id": "JoR-W2mkRVsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerger(nn.Module):\n",
        "    def __init__(self, dim, num_tokens_out):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** -0.5\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.queries = nn.Parameter(torch.randn(num_tokens_out, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x - b, n, d\n",
        "        x = self.norm(x)\n",
        "        sim = torch.matmul(self.queries, x.transpose(-1, -2)) * self.scale\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        return torch.matmul(attn, x)"
      ],
      "metadata": {
        "id": "onMuWMCoRZC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SE"
      ],
      "metadata": {
        "id": "D1Bs2iKjqu2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        reduction: int = 16\n",
        "    ) -> None:\n",
        "        super(SEBlock, self).__init__()\n",
        "\n",
        "        out_channels = in_channels // reduction\n",
        "\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, in_channels, kernel_size=1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z = self.squeeze(x)  # (batch_size, in_channels, 1, 1), eq.2\n",
        "        s = self.excitation(z)  # (batch_size, in_channels, 1, 1), eq.3\n",
        "        out = x * s  # channel-wise multiplication, eq. 4\n",
        "        return out"
      ],
      "metadata": {
        "id": "-eGlI139qxiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selective Kernel"
      ],
      "metadata": {
        "id": "ydkXUtiss0-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import List, Optional\n",
        "\n",
        "class SKConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: Optional[int] = None,\n",
        "        kernels: List[int] = [3, 5],\n",
        "        reduction: int = 16,\n",
        "        L: int = 32,\n",
        "        groups: int = 32\n",
        "    ) -> None:\n",
        "        super(SKConv, self).__init__()\n",
        "\n",
        "        d = max(in_channels // reduction, L)  # eq.4\n",
        "\n",
        "        self.M = len(kernels)\n",
        "\n",
        "        if out_channels is None:\n",
        "            out_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size = k,\n",
        "                    stride = 1,\n",
        "                    padding = (k - 1) // 2,\n",
        "                    groups = groups\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            for k in kernels\n",
        "        ])\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.fc_z = nn.Sequential(\n",
        "            nn.Linear(out_channels, d),\n",
        "            nn.BatchNorm1d(d),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_attn = nn.Linear(d, out_channels * self.M)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # ----- split -----\n",
        "        feats = torch.cat([conv(x).unsqueeze(1) for conv in self.convs], dim=1)  # (batch_size, M, out_channels, height, width)\n",
        "\n",
        "        # ----- fuse -----\n",
        "        # eq.1\n",
        "        U = torch.sum(feats, dim=1)  # (batch_size, out_channels, height, width)\n",
        "        # channel-wise statistics, eq.2\n",
        "        s = self.pool(U).squeeze(-1).squeeze(-1)  # (batch_size, out_channels)\n",
        "        # compact feature, eq.3\n",
        "        z = self.fc_z(s)  # (batch_size, d)\n",
        "\n",
        "        # ----- select -----\n",
        "        batch_size, out_channels = s.shape\n",
        "\n",
        "        # attention map, eq.5\n",
        "        score = self.fc_attn(z)  # (batch_size, M * out_channels)\n",
        "        score = score.view(batch_size, self.M, out_channels, 1, 1)  # (batch_size, M, out_channels, 1, 1)\n",
        "        att = self.softmax(score)\n",
        "\n",
        "        # fuse multiple branches, eq.6\n",
        "        out = torch.sum(att * feats, dim=1)  # (batch_size, out_channels, height, width)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yg_FxVTas8L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoPE"
      ],
      "metadata": {
        "id": "pdVUKxR207um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://nn.labml.ai/transformers/rope/index.html"
      ],
      "metadata": {
        "id": "tIJiIL8b0-zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionalEmbeddings(nn.Module):\n",
        "  def __init__(self, d: int, base: int = 10_000):\n",
        "      \"\"\"\n",
        "      * `d` is the number of features $d$\n",
        "      * `base` is the constant used for calculating $\\Theta$\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "\n",
        "      self.base = base\n",
        "      self.d = d\n",
        "      self.cos_cached = None\n",
        "      self.sin_cached = None\n",
        "\n",
        "  def _build_cache(self, x: torch.Tensor):\n",
        "      \"\"\"\n",
        "      Cache $\\cos$ and $\\sin$ values\n",
        "      \"\"\"\n",
        "      # Return if cache is already built\n",
        "      if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
        "          return\n",
        "\n",
        "      # Get sequence length\n",
        "      seq_len = x.shape[0]\n",
        "\n",
        "      # $\\Theta = {\\theta_i = 10000^{-\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
        "      theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device)\n",
        "\n",
        "      # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
        "      seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device)\n",
        "\n",
        "      # Calculate the product of position index and $\\theta_i$\n",
        "      idx_theta = torch.einsum('n,d->nd', seq_idx, theta)\n",
        "\n",
        "      # Concatenate so that for row $m$ we have\n",
        "      # $[m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}, m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}]$\n",
        "      idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n",
        "\n",
        "      # Cache them\n",
        "      self.cos_cached = idx_theta2.cos()[:, None, None, :]\n",
        "      self.sin_cached = idx_theta2.sin()[:, None, None, :]\n",
        "\n",
        "  def _neg_half(self, x: torch.Tensor):\n",
        "      # $\\frac{d}{2}$\n",
        "      d_2 = self.d // 2\n",
        "\n",
        "      # Calculate $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n",
        "      return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      \"\"\"\n",
        "      * `x` is the Tensor at the head of a key or a query with shape `[seq_len, batch_size, n_heads, d]`\n",
        "      \"\"\"\n",
        "      # Cache $\\cos$ and $\\sin$ values\n",
        "      self._build_cache(x)\n",
        "\n",
        "      # Split the features, we can choose to apply rotary embeddings only to a partial set of features.\n",
        "      x_rope, x_pass = x[..., :self.d], x[..., self.d:]\n",
        "\n",
        "      # Calculate\n",
        "      # $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n",
        "      neg_half_x = self._neg_half(x_rope)\n",
        "\n",
        "      # Calculate\n",
        "      #\n",
        "      # \\begin{align}\n",
        "      # \\begin{pmatrix}\n",
        "      # x^{(i)}_m \\cos m \\theta_i - x^{(i + \\frac{d}{2})}_m \\sin m \\theta_i \\\\\n",
        "      # x^{(i + \\frac{d}{2})}_m \\cos m\\theta_i + x^{(i)}_m \\sin m \\theta_i \\\\\n",
        "      # \\end{pmatrix} \\\\\n",
        "      # \\end{align}\n",
        "      #\n",
        "      # for $i \\in {1, 2, ..., \\frac{d}{2}}$\n",
        "      x_rope = (x_rope * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]])\n",
        "\n",
        "      #\n",
        "      return torch.cat((x_rope, x_pass), dim=-1)"
      ],
      "metadata": {
        "id": "Odi7TeJ60_7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla Attention"
      ],
      "metadata": {
        "id": "zEkAy6YB85hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8OLSBqm787mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the attention network proposed in [1] and [2].\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim : int\n",
        "        Size of the input tensor\n",
        "    References\n",
        "    ----------\n",
        "    1. \"`Neural Machine Translation by Jointly Learning to Align and Translate. \\\n",
        "            <https://arxiv.org/abs/1409.0473>`_\" Dzmitry Bahdanau, et al. ICLR 2015.\n",
        "    2. \"`Effective Approaches to Attention-based Neural Machine Translation. \\\n",
        "            <https://arxiv.org/abs/1508.04025>`_\" Minh-Thang Luong, et al. EMNLP 2015.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "    ) -> None:\n",
        "        super(VanillaAttention, self).__init__()\n",
        "\n",
        "        self.fc_align = nn.Linear(dim, dim)\n",
        "\n",
        "        self.fc_query = nn.Linear(dim, dim)\n",
        "        self.fc_value = nn.Linear(dim, dim)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self, query: torch.Tensor, key: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        query : torch.Tensor (batch_size, dim)\n",
        "            Query\n",
        "        key : torch.Tensor (batch_size, length, dim)\n",
        "            Key\n",
        "        Returns\n",
        "        -------\n",
        "        out : torch.Tensor (batch_size, dim)\n",
        "            Output tensor\n",
        "        att: torch.Tensor (batch_size, length)\n",
        "            Attention weights\n",
        "        \"\"\"\n",
        "\n",
        "        # alignment scores\n",
        "        score = self.fc_align(query)  # (batch_size, dim)\n",
        "        score = (key @ score.unsqueeze(2)).squeeze(2)  # (batch_size, length)\n",
        "\n",
        "        # attention weights\n",
        "        att = self.softmax(score)  # (batch_size, length)\n",
        "\n",
        "        # context vector (weighted value)\n",
        "        context = (att.unsqueeze(1) @ key).squeeze(1)  # (batch_size, dim)\n",
        "\n",
        "        # attention result\n",
        "        out = self.tanh(self.fc_value(context) + self.fc_query(query))\n",
        "\n",
        "        return out, att"
      ],
      "metadata": {
        "id": "Bv4z2HRE89Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "VXnPOD0U9KKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        self.xh = nn.Linear(input_size, hidden_size * 4, bias=bias)\n",
        "        self.hh = nn.Linear(hidden_size, hidden_size * 4, bias=bias)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "\n",
        "        # Inputs:\n",
        "        #       input: of shape (batch_size, input_size)\n",
        "        #       hx: of shape (batch_size, hidden_size)\n",
        "        # Outputs:\n",
        "        #       hy: of shape (batch_size, hidden_size)\n",
        "        #       cy: of shape (batch_size, hidden_size)\n",
        "\n",
        "        hx, cx = hx\n",
        "\n",
        "        gates = self.xh(input) + self.hh(hx)\n",
        "\n",
        "        # Get gates (i_t, f_t, g_t, o_t)\n",
        "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
        "\n",
        "        i_t = torch.sigmoid(input_gate)\n",
        "        f_t = torch.sigmoid(forget_gate)\n",
        "        g_t = torch.tanh(cell_gate)\n",
        "        o_t = torch.sigmoid(output_gate)\n",
        "\n",
        "        cy = cx * f_t + i_t * g_t\n",
        "\n",
        "        hy = o_t * torch.tanh(cy)\n",
        "\n",
        "\n",
        "        return (hy, cy)"
      ],
      "metadata": {
        "id": "c63DUkaQ9Loy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "4XuD3QzD9gye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5WqRuzfI9jxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_heads(x: torch.Tensor, n_heads: int) -> torch.Tensor:\n",
        "    batch_size, dim = x.size(0), x.size(-1)\n",
        "    x = x.view(batch_size, -1, n_heads, dim // n_heads)  # (batch_size, length, n_heads, d_head)\n",
        "    x = x.transpose(1, 2)  # (batch_size, n_heads, length, d_head)\n",
        "    return x\n",
        "\n",
        "def combine_heads(x: torch.Tensor) -> torch.Tensor:\n",
        "    batch_size, n_heads, d_head = x.size(0), x.size(1), x.size(3)\n",
        "    x = x.transpose(1, 2).contiguous().view(batch_size, -1, d_head * n_heads)  # (batch_size, length, n_heads * d_head)\n",
        "    return x\n",
        "\n",
        "def add_mask(x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "    if mask is not None:\n",
        "        if len(x.size()) == 4:\n",
        "            expanded_mask = mask.unsqueeze(1).unsqueeze(1)  # (batch_size, 1, 1, length)\n",
        "        x = x.masked_fill(expanded_mask.bool(), -np.inf)\n",
        "    return"
      ],
      "metadata": {
        "id": "x4cyr-5t9kWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, scale: float, dropout: float = 0.5) -> None:\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "        self.scale = scale\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = None if dropout is None else nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        Q: torch.Tensor,\n",
        "        K: torch.Tensor,\n",
        "        V: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # Q·K^T / sqrt(d_head)\n",
        "        score = torch.matmul(Q / self.scale, K.transpose(2, 3))  # (batch_size, n_heads, length, length)\n",
        "        score = add_mask(score, mask)\n",
        "\n",
        "        # eq.1: Attention(Q, K, V) = softmax(Q·K^T / sqrt(d_head))·V\n",
        "        att = self.softmax(score)  # (batch_size, n_heads, length, length)\n",
        "        att = att if self.dropout is None else self.dropout(att)\n",
        "        context = att @ V  # (batch_size, n_heads, length, d_head)\n",
        "\n",
        "        return context, att"
      ],
      "metadata": {
        "id": "Jcced4X79mgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        n_heads: int = 8,\n",
        "        dropout: Optional[float] = None\n",
        "    ) -> None:\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        assert dim % n_heads == 0\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = dim // n_heads\n",
        "\n",
        "        # linear projections\n",
        "        self.W_Q = nn.Linear(dim, n_heads * self.d_head)\n",
        "        self.W_K = nn.Linear(dim, n_heads * self.d_head)\n",
        "        self.W_V = nn.Linear(dim, n_heads * self.d_head)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scale = self.d_head ** 0.5  # scale factor\n",
        "        self.attention = ScaledDotProductAttention(scale=scale, dropout=dropout)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(dim)\n",
        "        self.fc = nn.Linear(n_heads * self.d_head, dim)\n",
        "\n",
        "        self.dropout = None if dropout is None else nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        Q = self.W_Q(x)  # (batch_size, length, n_heads * d_head)\n",
        "        K = self.W_K(x)\n",
        "        V = self.W_V(x)\n",
        "\n",
        "        Q, K, V = split_heads(Q, self.n_heads), split_heads(K, self.n_heads), split_heads(V, self.n_heads)\n",
        "        # (batch_size, n_heads, length, d_head)\n",
        "\n",
        "        context, _ = self.attention(Q, K, V, mask=mask)  # (batch_size, n_heads, length, d_head)\n",
        "        context = combine_heads(context)  # (batch_size, length, n_heads * d_head)\n",
        "\n",
        "        out = self.fc(context)  # (batch_size, length, dim)\n",
        "        out = out if self.dropout is None else self.dropout(out)\n",
        "\n",
        "        out = out + x  # residual connection\n",
        "        out = self.layer_norm(out)  # LayerNorm\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "LAfDVaK599UE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}